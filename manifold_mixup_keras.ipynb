{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjXVjs_Po4kN"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x \n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msoXgU6IpeK4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    " \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.image import resize\n",
    "\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHdntytgpsW6"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "EPOCHS = 20\n",
    "IMG_SIZE = 32\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEVP2BP2UxUn"
   },
   "source": [
    "# 1. Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3XZG3W-RsKEq",
    "outputId": "fdda5ee0-cdd2-43ed-d72a-164d00b4c595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "train_images = preprocess_input(train_images)\n",
    "test_images = preprocess_input(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCSfExYLPcC2"
   },
   "source": [
    "## 1.1 Subsetting images (If needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oba3He9XPgGe"
   },
   "outputs": [],
   "source": [
    "def count_unique(train_labels):\n",
    "  unique, counts = np.unique(train_labels, return_counts=True)\n",
    "  return dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMM0f7phFgi3"
   },
   "outputs": [],
   "source": [
    "num_per_class = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "DOg-Hin-NBcn",
    "outputId": "0d6aa75c-f1d5-4d3d-c564-cd2bd1f801eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 1000,\n",
       " 1.0: 1000,\n",
       " 2.0: 1000,\n",
       " 3.0: 1000,\n",
       " 4.0: 1000,\n",
       " 5.0: 1000,\n",
       " 6.0: 1000,\n",
       " 7.0: 1000,\n",
       " 8.0: 1000,\n",
       " 9.0: 1000}"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_train_images = np.empty((num_per_class*10, 32, 32, 3))\n",
    "subset_train_labels = np.empty((num_per_class*10, 1))\n",
    "\n",
    "for i in range(10): \n",
    "  indices = np.random.choice(np.where(train_labels == i)[0], num_per_class, replace = False)\n",
    "  subset_train_images[i*num_per_class: (i+1)*num_per_class] = train_images[indices]\n",
    "  subset_train_labels[i*num_per_class: (i+1)*num_per_class] = train_labels[indices]\n",
    "\n",
    "count_unique(subset_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6OVvbY9QP3Vc"
   },
   "source": [
    "## 1.2 Convert images into tf tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFH4RjoacdBh"
   },
   "source": [
    "### 1.2.1 Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PG7C-mb3cYX1",
    "outputId": "1030008e-bb37-473c-a6ba-f8457ccc8ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_labels = keras.utils.to_categorical(train_labels)\n",
    "test_labels = keras.utils.to_categorical(test_labels)\n",
    "\n",
    "train_images = tf.convert_to_tensor(train_images)\n",
    "test_images = tf.convert_to_tensor(test_images)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "test_labels = tf.convert_to_tensor(test_labels)\n",
    "\n",
    "print(train_images.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8FxNb4VcfLR"
   },
   "source": [
    "### 1.2.2 Subset of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuwDAsdnjyTG"
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p,], b[p,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVLW7ijEjzPn"
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = unison_shuffled_copies(subset_train_images, subset_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v34T_l0s7T1a",
    "outputId": "7ea0bb16-7631-467b-8c49-96a9ff25d51b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_labels = keras.utils.to_categorical(train_labels)\n",
    "test_labels = keras.utils.to_categorical(test_labels)\n",
    "\n",
    "train_images = tf.convert_to_tensor(train_images, dtype=tf.dtypes.float32)\n",
    "test_images = tf.convert_to_tensor(test_images)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "test_labels = tf.convert_to_tensor(test_labels)\n",
    "\n",
    "print(train_images.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsugA_GxU0z0"
   },
   "source": [
    "# 2. Base model - VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-T_Uf0vJqro4",
    "outputId": "6f419c51-d441-437a-83ca-a2775989031f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vgg_model = VGG16(input_shape = IMG_SHAPE,\n",
    "                  include_top = False,\n",
    "                  weights = 'imagenet')\n",
    "                  \n",
    "vgg_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCehuQSHAkPn"
   },
   "outputs": [],
   "source": [
    "layers_to_use = [l for l in vgg_model.layers]\n",
    "\n",
    "# layers_to_use.insert(1, layers.UpSampling2D(size=(7, 7)))\n",
    "layers_to_use.insert(1, layers.Lambda(lambda image: tf.image.resize(image, size=(224, 224))))\n",
    "layers_to_use.append(layers.Flatten())\n",
    "layers_to_use.append(layers.Dense(4096, activation = 'relu', name = 'first_dense'))\n",
    "layers_to_use.append(layers.Dense(4096, activation = 'relu', name = 'second_dense'))\n",
    "layers_to_use.append(layers.Dense(10, activation = 'softmax', name = 'output_layer'))\n",
    "\n",
    "base_model = keras.Sequential(\n",
    "                              layers_to_use\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-taDZnnmNzI"
   },
   "outputs": [],
   "source": [
    "mix_layer = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehkJ_cvrdBPG"
   },
   "outputs": [],
   "source": [
    "intermediate_layer_model = keras.Sequential(layers_to_use[0:mix_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqDZw472Z7m4"
   },
   "outputs": [],
   "source": [
    "output_model = keras.Sequential(layers_to_use[mix_layer:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klOnwjla2Y1-"
   },
   "source": [
    "# 3. Extended model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKjdlRcf469m"
   },
   "source": [
    "## 3.1 For mixup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nVQKRjPYNd6n"
   },
   "outputs": [],
   "source": [
    "input_layer = layers.Input(25088)\n",
    "first_dense = layers.Dense(4096, activation = 'relu')(input_layer)\n",
    "second_dense = layers.Dense(4096, activation = 'relu')(first_dense)\n",
    "output_layer = layers.Dense(10, activation = 'softmax')(second_dense)\n",
    "\n",
    "model = keras.Model(\n",
    "                    inputs = input_layer,\n",
    "                    outputs = output_layer\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LR_vYhe8LKX0"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/My Drive/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V7HHy5KjL31O",
    "outputId": "83ca1adb-d4f3-4fb7-8655-578f03ae834e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.8551999926567078\n"
     ]
    }
   ],
   "source": [
    "for x_batch_val, y_batch_val in val_dataset:\n",
    "    test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "print(\"Validation acc:\", (float(val_acc_metric.result())))\n",
    "val_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3tUR3l024-Fc"
   },
   "source": [
    "## 3.2 For regular training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-B72OBQ-YW4"
   },
   "outputs": [],
   "source": [
    "layers_to_use.extend((layers.Dense(4096, activation = 'relu'),\n",
    "                          layers.Dense(4096, activation = 'relu'),\n",
    "                          layers.Dense(10, activation = 'softmax')))\n",
    "\n",
    "model = keras.Sequential(\n",
    "                          layers_to_use \n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xo5Qsk9uOrTX"
   },
   "source": [
    "# 4. Training with Mixup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TRo9PuX6Lgz"
   },
   "source": [
    "## 4.1 Mixup with alpha for each sample of the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpE3lw9porsj"
   },
   "source": [
    "### 4.1.1 Mixup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSgOUPYcXkXi"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def data_mixup(data, labels):\n",
    "\n",
    "    '''This function performs mixup of data and labels.\n",
    "    It uses an alpha value for each sample in the batch\n",
    "    data = input data\n",
    "    labels = labels of input data\n",
    "    '''\n",
    "\n",
    "    num_data = int(data.shape[0]/2)\n",
    "\n",
    "    data = tf.reshape(data, shape=(num_data, 2, *[shape for shape in data.shape[1:]]))\n",
    "\n",
    "    labels = tf.reshape(labels, shape=(num_data, 2, 10))\n",
    "\n",
    "    alpha = np.random.beta(0.5, 0.5, (1, num_data))\n",
    "    alpha_mat = tf.convert_to_tensor(np.concatenate((alpha, (1-alpha)), axis=0), dtype = tf.dtypes.float32)\n",
    "\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for i in range(num_data):\n",
    "      alpha_mat[:,i]\n",
    "      data_list.append((tf.tensordot(data[i], alpha_mat[:,i], axes = [0, 0])))\n",
    "      labels_list.append((tf.tensordot(labels[i], alpha_mat[:,i], axes=[0, 0])))\n",
    "\n",
    "\n",
    "    mixed_data = tf.stack(data_list)\n",
    "    mixed_labels = tf.stack(labels_list)\n",
    "\n",
    "    return mixed_data, mixed_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Leqdycjzo2L2"
   },
   "source": [
    "### 4.1.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "do8gcwYRj3fU"
   },
   "outputs": [],
   "source": [
    "BATCH = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnZfOXqOneQ2"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(keras.optimizers.SGD(learning_rate = 0.001))\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-91KDbMGfP06"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        x = intermediate_layer_model(x)\n",
    "        x, y = data_mixup(x, y)\n",
    "        model_output = output_model(x, training=True)\n",
    "\n",
    "        loss_value = loss_fn(y, model_output)\n",
    "\n",
    "    grads = tape.gradient(loss_value, base_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, base_model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, model_output)\n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    model_output_val = base_model(x, training=False)\n",
    "    val_acc_metric.update_state(y, model_output_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96lUEFxH-kyx"
   },
   "outputs": [],
   "source": [
    "train_acc_history = np.empty(EPOCHS)\n",
    "val_acc_history = np.empty(EPOCHS)\n",
    "loss_value_history = np.empty(EPOCHS)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):   \n",
    "        \n",
    "        # x_batch_train = base_model(x_batch_train)\n",
    "        # x_batch_train_mixed, y_batch_train_mixed = data_mixup(x_batch_train, y_batch_train, mixup='manifold')   \n",
    "\n",
    "        # loss_value = train_step(x_batch_train_mixed, y_batch_train_mixed)\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "      \n",
    "\n",
    "        if step % 100 == 0 and step > 0:\n",
    "            print(\"Training loss (for one batch)\", (float(loss_value)))\n",
    "            print(\"Training accuracy so far\", float(train_acc_metric.result()))\n",
    "            print(\"Seen so far: %s samples\" % (step * BATCH))\n",
    "\n",
    "    train_acc_history[epoch] = train_acc_metric.result()\n",
    "    loss_value_history[epoch] = loss_value\n",
    "\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "    print(\"Validation acc:\", (float(val_acc_metric.result())))\n",
    "    print(\"Time taken:\", (time.time() - start_time))\n",
    "    val_acc_history[epoch] = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pk32Wjp-ac4A"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/content/drive/My Drive/Colab Notebooks/model_man_full_allalpha_20epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1y4z3OJKUepz"
   },
   "outputs": [],
   "source": [
    "# Saving history in csv. format\n",
    "\n",
    "hist_df = pd.DataFrame(history.history) \n",
    " \n",
    "hist_csv_file = '/content/drive/My Drive/Colab Notebooks/history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQevb5HAg1Jw"
   },
   "source": [
    "## 4.2 Single alpha mixup training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nR30bGt2oUcK"
   },
   "source": [
    "### 4.2.1 Mixup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BgY5UyAzSHA6"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def data_mixup_single_alpha(data, labels):\n",
    "\n",
    "    '''This function performs mixup of data and labels.\n",
    "    It uses an alpha value for each sample in the batch\n",
    "    data = input data\n",
    "    labels = labels of input data\n",
    "    '''\n",
    "\n",
    "    num_data = int(data.shape[0]/2)\n",
    "\n",
    "    data = tf.reshape(data, shape=(num_data, 2, *[shape for shape in data.shape[1:]]))\n",
    "      \n",
    "    labels = tf.reshape(labels, shape=(num_data, 2, 10))\n",
    "\n",
    "    # alpha = np.random.uniform(0, 1, (1))\n",
    "    alpha = np.random.beta(0.5, 0.5, (1))\n",
    "    alpha_mat = tf.convert_to_tensor(np.concatenate((alpha, (1-alpha)), axis=0), dtype = tf.dtypes.float32)\n",
    "\n",
    "    mixed_data = tf.tensordot(data, alpha_mat,axes=[1,0])\n",
    "    mixed_labels = tf.tensordot(labels, alpha_mat,axes=[1,0])\n",
    "\n",
    "\n",
    "    return mixed_data, mixed_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_jW9e2Job-V"
   },
   "source": [
    "### 4.2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfGVhEgoRhzA"
   },
   "outputs": [],
   "source": [
    "BATCH = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJiVd95FRiLW"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(keras.optimizers.SGD(learning_rate = 0.0001))\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fCijFwYUhFo"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        x = intermediate_layer_model(x)\n",
    "        x, y = data_mixup(x, y)\n",
    "        model_output = output_model(x, training=True)\n",
    "\n",
    "        loss_value = loss_fn(y, model_output)\n",
    "\n",
    "    grads = tape.gradient(loss_value, base_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, base_model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, model_output)\n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    model_output_val = base_model(x, training=False)\n",
    "    val_acc_metric.update_state(y, model_output_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTRzhoE37S7K"
   },
   "outputs": [],
   "source": [
    "train_acc_history = np.empty(EPOCHS)\n",
    "val_acc_history = np.empty(EPOCHS)\n",
    "loss_value_history = np.empty(EPOCHS)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):   \n",
    "\n",
    "        # x_batch_train_mixed, y_batch_train_mixed = data_mixup_single_alpha(x_batch_train, y_batch_train)   \n",
    "\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "      \n",
    "\n",
    "        if step % 100 == 0 and step > 0:\n",
    "            print(\"Training loss (for one batch)\", (float(loss_value)))\n",
    "            print(\"Training accuracy so far\", float(train_acc_metric.result()))\n",
    "            print(\"Seen so far: %s samples\" % (step * BATCH))\n",
    "\n",
    "    train_acc_history[epoch] = train_acc_metric.result()\n",
    "    loss_value_history[epoch] = loss_value\n",
    "\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "    print(\"Validation acc:\", (float(val_acc_metric.result())))\n",
    "    print(\"Time taken:\", (time.time() - start_time))\n",
    "    val_acc_history[epoch] = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZiRGPhHQIJc"
   },
   "outputs": [],
   "source": [
    "# Saving history in csv. format\n",
    "\n",
    "hist_df = pd.DataFrame(history.history) \n",
    " \n",
    "hist_csv_file = '/content/drive/My Drive/Colab Notebooks/history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iV2TulYQWC42"
   },
   "source": [
    "# 5. Training without mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSIoefba-6Xo"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = keras.optimizers.SGD(learning_rate = 0.001), \n",
    "              loss = keras.losses.CategoricalCrossentropy(), \n",
    "              metrics = keras.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJPbLJceG5hv"
   },
   "outputs": [],
   "source": [
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "                    train_images,\n",
    "                    train_labels, \n",
    "                    batch_size=64,\n",
    "                    epochs=1, \n",
    "                    callbacks=[tensorboard_callback],\n",
    "                    validation_data=(test_images, test_labels)\n",
    "                    )\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATSZiFe6alrW"
   },
   "outputs": [],
   "source": [
    "# Saving history in csv. format\n",
    "\n",
    "hist_df = pd.DataFrame(history.history) \n",
    " \n",
    "hist_csv_file = '/content/drive/My Drive/Colab Notebooks/history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPzjYSsy0Twm"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MN7EBc4D6HEA"
   },
   "source": [
    "# 6. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtMZ9fJIhrtT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHu1lCgFzbn_"
   },
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qY9RUmbS6iOd"
   },
   "outputs": [],
   "source": [
    "y = np.arange(1, 21)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(y, reg_pd['5000'], label='5000 per class')\n",
    "plt.plot(y, reg_pd['4000'], label='4000 per class')\n",
    "plt.plot(y, reg_pd['3000'], label='3000 per class')\n",
    "plt.plot(y, reg_pd['2000'], label='2000 per class')\n",
    "plt.plot(y, reg_pd['1000'], label='1000 per class')\n",
    "plt.annotate(s = str(0.860),  xy=(19, 0.87), fontsize=12)\n",
    "plt.annotate(s = str(0.856),  xy=(19, 0.85), fontsize=12)\n",
    "plt.annotate(s = str(0.822),  xy=(19, 0.83), fontsize=12)\n",
    "plt.annotate(s = str(0.816),  xy=(19, 0.80), fontsize=12)\n",
    "plt.annotate(s = str(0.770),  xy=(19, 0.77), fontsize=12)\n",
    "plt.xticks(y , ('1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'))\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy per epoch (Mixup)', fontsize=22)\n",
    "plt.ylabel('Accuracy in percent', fontsize=18)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylim([0.4, 1])\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zh1l8wTS8sNY"
   },
   "outputs": [],
   "source": [
    "y = np.arange(1, 21)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(y, noreg_pd['5000'], label='5000 per class')\n",
    "plt.plot(y, noreg_pd['4000'], label='4000 per class')\n",
    "plt.plot(y, noreg_pd['3000'], label='3000 per class')\n",
    "plt.plot(y, noreg_pd['2000'], label='2000 per class')\n",
    "plt.plot(y, noreg_pd['1000'], label='1000 per class')\n",
    "plt.annotate(s = str(0.864),  xy=(19, 0.87), fontsize=12)\n",
    "plt.annotate(s = str(0.858),  xy=(19, 0.85), fontsize=12)\n",
    "plt.annotate(s = str(0.842),  xy=(19, 0.829), fontsize=12)\n",
    "plt.annotate(s = str(0.833),  xy=(19, 0.81), fontsize=12)\n",
    "plt.annotate(s = str(0.791),  xy=(19, 0.78), fontsize=12)\n",
    "plt.xticks(y, ('1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'))\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.title('Accuracy per epoch (No Mixup)', fontsize=22)\n",
    "plt.ylabel('Accuracy in percent', fontsize=18)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylim([0.4, 1])\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gx5sO6dC2AX"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot([5000, 4000, 3000, 2000, 1000], final['No Mixup'][0:5], label='No Mixup')\n",
    "plt.plot([5000, 4000, 3000, 2000, 1000], final['Mixup'][0:5], label='Mixup')\n",
    "plt.annotate(s = str(0.860),  xy=(4930, 0.857), fontsize=12)\n",
    "plt.annotate(s = str(0.856),  xy=(4000, 0.850), fontsize=12)\n",
    "plt.annotate(s = str(0.822),  xy=(3000, 0.822), fontsize=12)\n",
    "plt.annotate(s = str(0.816),  xy=(2000, 0.81), fontsize=12)\n",
    "plt.annotate(s = str(0.770),  xy=(1000, 0.77), fontsize=12)\n",
    "plt.annotate(s = str(0.864),  xy=(4930, 0.868), fontsize=12)\n",
    "plt.annotate(s = str(0.858),  xy=(4000, 0.863), fontsize=12)\n",
    "plt.annotate(s = str(0.842),  xy=(3000, 0.845), fontsize=12)\n",
    "plt.annotate(s = str(0.833),  xy=(2000, 0.835), fontsize=12)\n",
    "plt.annotate(s = str(0.791),  xy=(1000, 0.792), fontsize=12)\n",
    "plt.xticks([5000, 4000, 3000, 2000, 1000], ('5000','4000', '3000', '2000', '1000'))\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.title('Last epoch accuracy per subset size', fontsize=22)\n",
    "plt.ylabel('Accuracy in percent', fontsize=18)\n",
    "plt.xlabel('Subset size', fontsize=18)\n",
    "plt.ylim([0.6, 1])\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gCSfExYLPcC2",
    "g8FxNb4VcfLR",
    "GKjdlRcf469m",
    "3tUR3l024-Fc"
   ],
   "name": "amtns-2nd layer mix.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
